{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Aggregation ETL Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will aggregate IoT Event data stored in Cloudant and save the aggregated information in a dashDB Data Warehouse. It uses Spark Data Frames to store the information.\n",
    "\n",
    "The notebook is divided into several sections:\n",
    "\n",
    "1. Common Declarations\n",
    "2. Database credentials\n",
    "3. Read event data from Cloudant into Spark Data Frames\n",
    "4. Aggregate event data using Data Frame operations\n",
    "5. Load aggregated event data in the Data Frames to dashDB Tables\n",
    "6. Merge aggregated event data into target dashDB tables using ibmdbpy and SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following fragment of code sets the scene by defining imports, global variables and connection functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as sqlfunc\n",
    "\n",
    "#debug\n",
    "debug = 1\n",
    "\n",
    "#Initializations\n",
    "# Define Spark configuration\n",
    "conf = SparkConf()\n",
    "\n",
    "#Please don't modify this function\n",
    "def readDataFrameFromCloudant(host,user,pw,database):\n",
    "    cloudantdata=spark.read.format(\"com.cloudant.spark\"). \\\n",
    "    option(\"cloudant.host\",host). \\\n",
    "    option(\"cloudant.username\", user). \\\n",
    "    option(\"cloudant.password\", pw). \\\n",
    "    load(database)\n",
    "\n",
    "    cloudantdata.createOrReplaceTempView(\"elevator_telemetrics\")\n",
    "    return cloudantdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Base Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iotp_ep8hlc_elevator_history_2017_09_25\n"
     ]
    }
   ],
   "source": [
    "# The following code is used to access your data and contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "\n",
    "# Determine if Cloudant bucket from yesterday should be used\n",
    "\n",
    "bucket_prefix = 'iotp_ep8hlc_elevator_history'\n",
    "\n",
    "import datetime\n",
    "yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "yesterdays_bucket_name = bucket_prefix + '_' + yesterday.strftime('%Y_%m_%d')\n",
    "\n",
    "if debug : print yesterdays_bucket_name\n",
    "\n",
    "# @hidden_cell\n",
    "credentials_cloudant = {\n",
    "  'hostname':'<hostname>',\n",
    "  'user':'<user>',\n",
    "  'password':'<password>',\n",
    "  'database':'<database bucket>'\n",
    "}\n",
    "\n",
    "credentials_dashdb = {\n",
    "    'host' : '<host>',\n",
    "    'port' : '50000',\n",
    "    'database' : 'BLUDB',\n",
    "    'jdbcurl': '<jdbc>',\n",
    "    'username': 'dash6769',\n",
    "    'password': '<password>'\n",
    "}\n",
    "\n",
    "debug = 0\n",
    "credentials_cloudant[\"database\"] = yesterdays_bucket_name\n",
    "if debug : print(credentials_cloudant[\"database\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Event Data from Cloudant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from Cloudant and associate it with a Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+----------+---------+------+--------------------+\n",
      "|                 _id|                _rev|                data|  deviceId|deviceType|eventType|format|           timestamp|\n",
      "+--------------------+--------------------+--------------------+----------+----------+---------+------+--------------------+\n",
      "|00000930-33e1-11e...|1-436a34550c3fbba...|[[4.0,73,1,1.0,0....|Elevator02|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|00016aa0-3a35-11e...|1-ba05047d8a33b08...|[[0.0,92,1,0.0,0....|Elevator01|  Elevator|   Status|  json|2017-05-16T12:41:...|\n",
      "|00026f70-362c-11e...|1-3fc9a3a790faae9...|[[0.0,94,1,0.0,0....|Elevator02|  Elevator|   Status|  json|2017-05-11T09:27:...|\n",
      "|0002f140-3a35-11e...|1-7fa9ac529db082f...|[[0.0,74,1,2.0,0....|Elevator02|  Elevator|   Status|  json|2017-05-16T12:41:...|\n",
      "|00033d80-33e1-11e...|1-fbaaa184b48d0f7...|[[4.0,85,1,3.0,0....|Elevator03|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|00044430-362c-11e...|1-ac3093a45a15e71...|[[0.0,84,1,0.0,0....|Elevator03|  Elevator|   Status|  json|2017-05-11T09:27:...|\n",
      "|00065950-3627-11e...|1-08f6982f18f6481...|[[0.0,83,1,0.0,0....|Elevator07|  Elevator|   Status|  json|2017-05-11T08:51:...|\n",
      "|00069180-33e4-11e...|1-5d54e1778b50ded...|[[0.0,80,1,3.0,0....|Elevator10|  Elevator|   Status|  json|2017-05-08T11:46:...|\n",
      "|000828c1-3a32-11e...|1-7fbd2ae24ddf6a9...|[[4.0,92,1,1.0,0....|Elevator01|  Elevator|   Status|  json|2017-05-16T12:20:...|\n",
      "|000ba5f0-3626-11e...|1-95bc1028ed1f746...|[[0.0,91,1,0.0,0....|Elevator06|  Elevator|   Status|  json|2017-05-11T08:44:...|\n",
      "|000cda70-33e1-11e...|1-33d21782e154e74...|[[14.0,87,1,0.0,0...|Elevator04|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|000d3a50-362b-11e...|1-f24c1c4096af40f...|[[0.0,89,1,2.0,0....|Elevator01|  Elevator|   Status|  json|2017-05-11T09:19:...|\n",
      "|000d4fa0-33e1-11e...|1-992be3de14b1a85...|[[0.0,88,1,4.0,0....|Elevator05|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|000eb330-3626-11e...|1-3d0497a9269d5c9...|[[0.0,83,1,4.0,0....|Elevator07|  Elevator|   Status|  json|2017-05-11T08:44:...|\n",
      "|000fc4a1-3626-11e...|1-a7e886ac095eb00...|[[0.0,88,1,3.0,0....|Elevator10|  Elevator|   Status|  json|2017-05-11T08:44:...|\n",
      "|00106b70-3627-11e...|1-fcef176b0553d3a...|[[0.0,93,1,4.0,0....|Elevator08|  Elevator|   Status|  json|2017-05-11T08:51:...|\n",
      "|0010bff0-362f-11e...|1-ab191d8df89a13e...|[[0.0,74,1,0.0,1....|Elevator09|  Elevator|   Status|  json|2017-05-11T09:48:...|\n",
      "|0011c3d0-33de-11e...|1-1bdc32dff3f1b71...|[[0.0,87,1,1.0,0....|Elevator01|  Elevator|   Status|  json|2017-05-08T11:03:...|\n",
      "|00127fc0-33e1-11e...|1-a0fbf1aab7fb5bc...|[[0.0,79,1,3.0,0....|Elevator07|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|0012f6d0-3a35-11e...|1-627561233f12c3d...|[[4.0,82,1,3.0,0....|Elevator03|  Elevator|   Status|  json|2017-05-16T12:41:...|\n",
      "+--------------------+--------------------+--------------------+----------+----------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdf=readDataFrameFromCloudant(credentials_cloudant['hostname'], credentials_cloudant['user'], credentials_cloudant['password'], credentials_cloudant['database'])\n",
    "if debug : cdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns date and motor temperature as integer to the Data Frame using SQL functions. For a complete list see:\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+------------------+----------+------------+\n",
      "|  deviceId|deviceType|           timestamp|         motorTemp|      date|motorTempInt|\n",
      "+----------+----------+--------------------+------------------+----------+------------+\n",
      "|Elevator02|  Elevator|2017-05-08T11:25:...|166.80070000000157|2017-05-08|       167.0|\n",
      "|Elevator01|  Elevator|2017-05-16T12:41:...|166.77350000000345|2017-05-16|       167.0|\n",
      "|Elevator02|  Elevator|2017-05-11T09:27:...|173.50340000000372|2017-05-11|       174.0|\n",
      "|Elevator02|  Elevator|2017-05-16T12:41:...|166.73740000000333|2017-05-16|       167.0|\n",
      "|Elevator03|  Elevator|2017-05-08T11:25:...| 155.1188000000015|2017-05-08|       155.0|\n",
      "|Elevator03|  Elevator|2017-05-11T09:27:...|166.31770000000355|2017-05-11|       166.0|\n",
      "|Elevator07|  Elevator|2017-05-11T08:51:...|165.14320000000126|2017-05-11|       165.0|\n",
      "|Elevator10|  Elevator|2017-05-08T11:46:...|168.11020000000303|2017-05-08|       168.0|\n",
      "|Elevator01|  Elevator|2017-05-16T12:20:...|164.07350000000207|2017-05-16|       164.0|\n",
      "|Elevator06|  Elevator|2017-05-11T08:44:...|156.10780000000076|2017-05-11|       156.0|\n",
      "|Elevator04|  Elevator|2017-05-08T11:25:...| 177.1048000000049|2017-05-08|       177.0|\n",
      "|Elevator01|  Elevator|2017-05-11T09:19:...|156.48350000000306|2017-05-11|       156.0|\n",
      "|Elevator05|  Elevator|2017-05-08T11:25:...|164.34120000000158|2017-05-08|       164.0|\n",
      "|Elevator07|  Elevator|2017-05-11T08:44:...|164.18320000000077|2017-05-11|       164.0|\n",
      "|Elevator10|  Elevator|2017-05-11T08:44:...|152.18000000000072|2017-05-11|       152.0|\n",
      "|Elevator08|  Elevator|2017-05-11T08:51:...|158.92400000000129|2017-05-11|       159.0|\n",
      "|Elevator09|  Elevator|2017-05-11T09:48:...|171.76160000000493|2017-05-11|       172.0|\n",
      "|Elevator01|  Elevator|2017-05-08T11:03:...|151.38450000000017|2017-05-08|       151.0|\n",
      "|Elevator07|  Elevator|2017-05-08T11:25:...|156.65510000000157|2017-05-08|       157.0|\n",
      "|Elevator03|  Elevator|2017-05-16T12:41:...|163.86150000000322|2017-05-16|       164.0|\n",
      "+----------+----------+--------------------+------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.functions import rint\n",
    "\n",
    "cdf_select = cdf.select(\"deviceId\",\"deviceType\",\"timestamp\", \"data.d.motorTemp\")\n",
    "cdf_date = cdf_select.withColumn(\"date\", substring('timestamp',1,10))\n",
    "cdf_mtemp = cdf_date.withColumn(\"motorTempInt\", rint('motorTemp'))\n",
    "\n",
    "cdf_mtemp.cache()\n",
    "\n",
    "if debug : cdf_mtemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Event Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the event data using groupBy and agg. The operations are documented here:\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+------------------+\n",
      "|  deviceId|deviceType|      date|    min(motorTemp)|    avg(motorTemp)|    max(motorTemp)|\n",
      "+----------+----------+----------+------------------+------------------+------------------+\n",
      "|Elevator02|  Elevator|2017-05-08|          163.7207|166.94495418060362|170.10070000000326|\n",
      "|Elevator09|  Elevator|2017-05-16|          159.3389|162.97127758112282|166.67890000000375|\n",
      "|Elevator05|  Elevator|2017-05-08|          161.2412|164.43060221402374|167.78120000000334|\n",
      "|Elevator01|  Elevator|2017-05-11|          150.5035|156.09050301205096|161.60350000000568|\n",
      "|Elevator08|  Elevator|2017-05-08|          151.5074| 154.5347800738023|157.68740000000315|\n",
      "|Elevator06|  Elevator|2017-05-16|161.39589999999998|164.91468609310735|168.39590000000356|\n",
      "|Elevator06|  Elevator|2017-05-08|157.02169999999998|160.09710961408416| 163.1017000000031|\n",
      "|Elevator01|  Elevator|2017-05-08|          151.0245|172.97757808311002|             214.0|\n",
      "|Elevator08|  Elevator|2017-05-16|155.66889999999998|159.26504877024774|162.80890000000363|\n",
      "|Elevator09|  Elevator|2017-05-08|151.21949999999998| 154.4492926421421|157.65950000000328|\n",
      "|Elevator07|  Elevator|2017-05-08|          153.5751|156.73402250922678|159.95510000000326|\n",
      "|Elevator10|  Elevator|2017-05-11|            150.76|156.34268173258283|162.02000000000575|\n",
      "|Elevator10|  Elevator|2017-05-16|169.60629999999998|173.18122007104978| 176.8663000000037|\n",
      "|Elevator04|  Elevator|2017-05-16|151.98049999999998|155.64550299940197| 159.0805000000036|\n",
      "|Elevator03|  Elevator|2017-05-08|152.13879999999997|155.22937718120963| 158.4388000000032|\n",
      "|Elevator10|  Elevator|2017-05-08|          162.1702| 165.3467211551393| 168.6302000000033|\n",
      "|Elevator02|  Elevator|2017-05-16|160.17739999999998|163.77602028302067| 167.4374000000037|\n",
      "|Elevator05|  Elevator|2017-05-11|          156.3576|172.28645425287763|             180.0|\n",
      "|Elevator07|  Elevator|2017-05-11|          162.6832|168.40398160919835|174.06320000000582|\n",
      "|Elevator04|  Elevator|2017-05-11|          158.1572| 163.8947335120672| 169.2972000000057|\n",
      "+----------+----------+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "cdf_aggtemp = cdf_mtemp.groupBy('deviceId','deviceType','date').agg(min('motorTemp'),avg('motorTemp'),max('motorTemp'))\n",
    "if debug : cdf_aggtemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the event data to determine the distribution of temperatures for each elevator by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------+-------------------+\n",
      "|  deviceId|deviceType|      date|motorTempInt|count(motorTempInt)|\n",
      "+----------+----------+----------+------------+-------------------+\n",
      "|Elevator08|  Elevator|2017-05-16|       161.0|                250|\n",
      "|Elevator08|  Elevator|2017-05-11|       157.0|                199|\n",
      "|Elevator07|  Elevator|2017-05-08|       156.0|                219|\n",
      "|Elevator02|  Elevator|2017-05-16|       161.0|                237|\n",
      "|Elevator07|  Elevator|2017-05-16|       168.0|                250|\n",
      "|Elevator06|  Elevator|2017-05-11|       165.0|                231|\n",
      "|Elevator07|  Elevator|2017-05-08|       160.0|                 88|\n",
      "|Elevator02|  Elevator|2017-05-08|       167.0|                231|\n",
      "|Elevator05|  Elevator|2017-05-11|       179.0|                 75|\n",
      "|Elevator10|  Elevator|2017-05-16|       174.0|                237|\n",
      "|Elevator01|  Elevator|2017-05-16|       164.0|                225|\n",
      "|Elevator10|  Elevator|2017-05-16|       177.0|                 87|\n",
      "|Elevator07|  Elevator|2017-05-08|       154.0|                194|\n",
      "|Elevator09|  Elevator|2017-05-16|       164.0|                231|\n",
      "|Elevator04|  Elevator|2017-05-08|       171.0|                 70|\n",
      "|Elevator09|  Elevator|2017-05-08|       156.0|                230|\n",
      "|Elevator02|  Elevator|2017-05-16|       165.0|                237|\n",
      "|Elevator09|  Elevator|2017-05-11|       167.0|                250|\n",
      "|Elevator03|  Elevator|2017-05-08|       154.0|                237|\n",
      "|Elevator05|  Elevator|2017-05-16|       156.0|                237|\n",
      "+----------+----------+----------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "cdf_distrtemp = cdf_mtemp.groupBy('deviceId','deviceType','date','motorTempInt').agg(count('motorTempInt'))\n",
    "if debug : cdf_distrtemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Aggregated Data to Data Warehouse Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to save the data to the Data Warehouse. The lines of code relevant for doing that have been borrowed from: https://apsportal.ibm.com/analytics/notebooks/55e7fe57-c002-4180-bfa3-83fc6466a875/view?access_token=1792df00fbbcf6df08f73bec0055b34e988d323eea59aafc6cd12d2a4ea86ed0\n",
    "Unlike mentioned in that tutorial, it is no longer neded to install pixiedust and a custom JDBC driver dialect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops!\n"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "try:\n",
    " cdf_aggtemp.write.jdbc(credentials_dashdb[\"jdbcurl\"], \\\n",
    "                       \"TEMP_DATAFRAME_AGGR_EVENT_DATA\", \\\n",
    "                       properties = {\"user\" : credentials_dashdb[\"username\"], \"password\" : credentials_dashdb[\"password\"]}, \\\n",
    "                       mode=\"overwrite\");       \n",
    "except Py4JJavaError:         \n",
    "    print(\"Oops!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf_aggtemp.write.jdbc(credentials_dashdb[\"jdbcurl\"], \\\n",
    "                       \"TEMP_DATAFRAME_AGGR_EVENT_DATA\", \\\n",
    "                       properties = {\"user\" : credentials_dashdb[\"username\"], \"password\" : credentials_dashdb[\"password\"]}, \\\n",
    "                       mode=\"overwrite\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf_distrtemp.write.jdbc(credentials_dashdb[\"jdbcurl\"], \"TEMP_DATAFRAME_DISTR_EVENT_DATA\", properties = {\"user\" : credentials_dashdb[\"username\"], \"password\" : credentials_dashdb[\"password\"]}, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Aggregated Data to Target Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregated data has been stored in a temporary table named TEMP_DATAFRAME_AGGR_EVENT_DATA. From there it needs to be copied to the table named ELEVATOR_EVENTS_AGGREGATED_BY_DAY that holds all aggregated data from previous runs of the ETL job. In this notebook we will use ibmdpy library to create a connection to dashdb as described in: \n",
    "https://apsportal.ibm.com/analytics/notebooks/5a59ba9b-02b2-40e4-b955-9727cb68c88b/view?access_token=09240b783432f1a62004bcc82b48a7aed07afc401e2f94a77c7e087b74d8c053\n",
    "\n",
    "Having established the connection, an SQL statement will be submitted that merges the content of the table TEMP_DATAFRAME_DISTR_EVENT_DATA to the table ELEVATOR_EVENTS_AGGREGATED_BY_DAY using SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ibmdbpy\n",
    "from ibmdbpy import IdaDataBase\n",
    "idadb = idadb = IdaDataBase(dsn=\"DASHDB;Database=BLUDB;Hostname=\" + credentials_dashdb[\"host\"] + \";Port=\" + credentials_dashdb[\"port\"] + \";PROTOCOL=TCPIP;UID=\" + credentials_dashdb[\"username\"] + \";PWD=\" + credentials_dashdb[\"password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recs before: 30\n",
      "Number of recs after: 30\n"
     ]
    }
   ],
   "source": [
    "from ibmdbpy import IdaDataFrame\n",
    "\n",
    "numberofRecsBefore = idadb.ida_scalar_query(\"SELECT count(*) FROM ELEVATOR_EVENTS_AGGREGATED_BY_DAY\")\n",
    "print(\"Number of recs before: \" + str(numberofRecsBefore))\n",
    "\n",
    "query =         'MERGE INTO ELEVATOR_EVENTS_AGGREGATED_BY_DAY t '\n",
    "query = query + 'USING (SELECT s.\"deviceId\" AS DEVICEID, s.\"deviceType\" AS DEVICETYPE, DATE(s.\"date\") AS DATE, ROUND(\"min(motorTemp)\",0) AS MINMOTORTEMP, ROUND(\"avg(motorTemp)\",0) AS AVGMOTORTEMP, ROUND(\"max(motorTemp)\",0) AS MAXMOTORTEMP '\n",
    "query = query + 'FROM TEMP_DATAFRAME_AGGR_EVENT_DATA s) e ON (e.DATE=t.DATE AND e.DEVICEID = t.DEVICEID) '\n",
    "query = query + 'WHEN MATCHED THEN UPDATE SET t.MINMOTORTEMP = e.MINMOTORTEMP, t.AVGMOTORTEMP = e.AVGMOTORTEMP, t.MAXMOTORTEMP = e.MAXMOTORTEMP ' \n",
    "query = query + 'WHEN NOT MATCHED THEN INSERT (DATE, DEVICEID, DEVICETYPE, MINMOTORTEMP, AVGMOTORTEMP, MAXMOTORTEMP) VALUES (e.DATE, e.DEVICEID, e.DEVICETYPE, e.MINMOTORTEMP,e.AVGMOTORTEMP,e.MAXMOTORTEMP);'\n",
    "\n",
    "result = idadb.ida_query(query)\n",
    "\n",
    "numberofRecsAfter = idadb.ida_scalar_query(\"SELECT count(*) FROM ELEVATOR_EVENTS_AGGREGATED_BY_DAY\")\n",
    "print(\"Number of recs after: \" + str(numberofRecsAfter))\n",
    "\n",
    "idadb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
