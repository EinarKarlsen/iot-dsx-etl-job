{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Aggregation ETL Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will aggregate IoT Event data stored in Cloudant and save the aggregated information in a dashDB Data Warehouse. It uses Spark Data Frames to store the information.\n",
    "\n",
    "The notebook is divided into several sections:\n",
    "\n",
    "1. Common Declaration\n",
    "2. Read event data from Cloudant into Spark Data Frames\n",
    "3. Aggregate event data using Data Frame operations\n",
    "4. Move aggregated event data in the Data Frames into dashDB Tables\n",
    "5. Merge aggregated event data into target dashDB tables using ibmdbpy and SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following fragment of code sets the scene by defining imports, global variables and connection functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as sqlfunc\n",
    "\n",
    "#debug\n",
    "debug = 1\n",
    "\n",
    "#Initializations\n",
    "# Define Spark configuration\n",
    "conf = SparkConf()\n",
    "\n",
    "# Initialize a SparkContext and SQLContext\n",
    "#sc = SparkContext(conf=conf)\n",
    "#sql_ctx = SQLContext(sc)\n",
    "\n",
    "#Please don't modify this function\n",
    "def readDataFrameFromCloudant(host,user,pw,database):\n",
    "    cloudantdata=spark.read.format(\"com.cloudant.spark\"). \\\n",
    "    option(\"cloudant.host\",host). \\\n",
    "    option(\"cloudant.username\", user). \\\n",
    "    option(\"cloudant.password\", pw). \\\n",
    "    load(database)\n",
    "\n",
    "    cloudantdata.createOrReplaceTempView(\"elevator_telemetrics\")\n",
    "    return cloudantdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Event Data from Cloudant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials for connecting to Cloudant DB. You will need to insert your credentials below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-22 03:26:28.992970\n",
      "2017-09-21 03:26:28.993516\n",
      "2017_09_21\n"
     ]
    }
   ],
   "source": [
    "# @hidden_cell\n",
    "credentials_cloudant = {\n",
    "  'hostname':'<hostname>',\n",
    "  'user':'<user>',\n",
    "  'password':'<password>',\n",
    "  'database':'<database bucket>'\n",
    "}\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print now\n",
    "yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "print yesterday\n",
    "\n",
    "print yesterday.strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts data from Cloudant and associates it with a Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+----------+---------+------+--------------------+\n",
      "|                 _id|                _rev|                data|  deviceId|deviceType|eventType|format|           timestamp|\n",
      "+--------------------+--------------------+--------------------+----------+----------+---------+------+--------------------+\n",
      "|00000930-33e1-11e...|1-436a34550c3fbba...|[[4.0,73,1,1.0,0....|Elevator02|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|00016aa0-3a35-11e...|1-ba05047d8a33b08...|[[0.0,92,1,0.0,0....|Elevator01|  Elevator|   Status|  json|2017-05-16T12:41:...|\n",
      "|00026f70-362c-11e...|1-3fc9a3a790faae9...|[[0.0,94,1,0.0,0....|Elevator02|  Elevator|   Status|  json|2017-05-11T09:27:...|\n",
      "|0002f140-3a35-11e...|1-7fa9ac529db082f...|[[0.0,74,1,2.0,0....|Elevator02|  Elevator|   Status|  json|2017-05-16T12:41:...|\n",
      "|00033d80-33e1-11e...|1-fbaaa184b48d0f7...|[[4.0,85,1,3.0,0....|Elevator03|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|00044430-362c-11e...|1-ac3093a45a15e71...|[[0.0,84,1,0.0,0....|Elevator03|  Elevator|   Status|  json|2017-05-11T09:27:...|\n",
      "|00065950-3627-11e...|1-08f6982f18f6481...|[[0.0,83,1,0.0,0....|Elevator07|  Elevator|   Status|  json|2017-05-11T08:51:...|\n",
      "|00069180-33e4-11e...|1-5d54e1778b50ded...|[[0.0,80,1,3.0,0....|Elevator10|  Elevator|   Status|  json|2017-05-08T11:46:...|\n",
      "|000828c1-3a32-11e...|1-7fbd2ae24ddf6a9...|[[4.0,92,1,1.0,0....|Elevator01|  Elevator|   Status|  json|2017-05-16T12:20:...|\n",
      "|000ba5f0-3626-11e...|1-95bc1028ed1f746...|[[0.0,91,1,0.0,0....|Elevator06|  Elevator|   Status|  json|2017-05-11T08:44:...|\n",
      "|000cda70-33e1-11e...|1-33d21782e154e74...|[[14.0,87,1,0.0,0...|Elevator04|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|000d3a50-362b-11e...|1-f24c1c4096af40f...|[[0.0,89,1,2.0,0....|Elevator01|  Elevator|   Status|  json|2017-05-11T09:19:...|\n",
      "|000d4fa0-33e1-11e...|1-992be3de14b1a85...|[[0.0,88,1,4.0,0....|Elevator05|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|000eb330-3626-11e...|1-3d0497a9269d5c9...|[[0.0,83,1,4.0,0....|Elevator07|  Elevator|   Status|  json|2017-05-11T08:44:...|\n",
      "|000fc4a1-3626-11e...|1-a7e886ac095eb00...|[[0.0,88,1,3.0,0....|Elevator10|  Elevator|   Status|  json|2017-05-11T08:44:...|\n",
      "|00106b70-3627-11e...|1-fcef176b0553d3a...|[[0.0,93,1,4.0,0....|Elevator08|  Elevator|   Status|  json|2017-05-11T08:51:...|\n",
      "|0010bff0-362f-11e...|1-ab191d8df89a13e...|[[0.0,74,1,0.0,1....|Elevator09|  Elevator|   Status|  json|2017-05-11T09:48:...|\n",
      "|0011c3d0-33de-11e...|1-1bdc32dff3f1b71...|[[0.0,87,1,1.0,0....|Elevator01|  Elevator|   Status|  json|2017-05-08T11:03:...|\n",
      "|00127fc0-33e1-11e...|1-a0fbf1aab7fb5bc...|[[0.0,79,1,3.0,0....|Elevator07|  Elevator|   Status|  json|2017-05-08T11:25:...|\n",
      "|0012f6d0-3a35-11e...|1-627561233f12c3d...|[[4.0,82,1,3.0,0....|Elevator03|  Elevator|   Status|  json|2017-05-16T12:41:...|\n",
      "+--------------------+--------------------+--------------------+----------+----------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdf=readDataFrameFromCloudant(credentials_cloudant['hostname'], credentials_cloudant['user'], credentials_cloudant['password'], credentials_cloudant['database'])\n",
    "if debug : cdf.show()\n",
    "#spark.sql(\"SELECT deviceId, deviceType, timestamp, data.d.motorTemp from elevator_telemetrics\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code adds a column date and motor temperature as integer to the Data Frame using SQL functions. For a complete list see:\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+------------------+----------+------------+\n",
      "|  deviceId|deviceType|           timestamp|         motorTemp|      date|motorTempInt|\n",
      "+----------+----------+--------------------+------------------+----------+------------+\n",
      "|Elevator02|  Elevator|2017-05-08T11:25:...|166.80070000000157|2017-05-08|       167.0|\n",
      "|Elevator01|  Elevator|2017-05-16T12:41:...|166.77350000000345|2017-05-16|       167.0|\n",
      "|Elevator02|  Elevator|2017-05-11T09:27:...|173.50340000000372|2017-05-11|       174.0|\n",
      "|Elevator02|  Elevator|2017-05-16T12:41:...|166.73740000000333|2017-05-16|       167.0|\n",
      "|Elevator03|  Elevator|2017-05-08T11:25:...| 155.1188000000015|2017-05-08|       155.0|\n",
      "|Elevator03|  Elevator|2017-05-11T09:27:...|166.31770000000355|2017-05-11|       166.0|\n",
      "|Elevator07|  Elevator|2017-05-11T08:51:...|165.14320000000126|2017-05-11|       165.0|\n",
      "|Elevator10|  Elevator|2017-05-08T11:46:...|168.11020000000303|2017-05-08|       168.0|\n",
      "|Elevator01|  Elevator|2017-05-16T12:20:...|164.07350000000207|2017-05-16|       164.0|\n",
      "|Elevator06|  Elevator|2017-05-11T08:44:...|156.10780000000076|2017-05-11|       156.0|\n",
      "|Elevator04|  Elevator|2017-05-08T11:25:...| 177.1048000000049|2017-05-08|       177.0|\n",
      "|Elevator01|  Elevator|2017-05-11T09:19:...|156.48350000000306|2017-05-11|       156.0|\n",
      "|Elevator05|  Elevator|2017-05-08T11:25:...|164.34120000000158|2017-05-08|       164.0|\n",
      "|Elevator07|  Elevator|2017-05-11T08:44:...|164.18320000000077|2017-05-11|       164.0|\n",
      "|Elevator10|  Elevator|2017-05-11T08:44:...|152.18000000000072|2017-05-11|       152.0|\n",
      "|Elevator08|  Elevator|2017-05-11T08:51:...|158.92400000000129|2017-05-11|       159.0|\n",
      "|Elevator09|  Elevator|2017-05-11T09:48:...|171.76160000000493|2017-05-11|       172.0|\n",
      "|Elevator01|  Elevator|2017-05-08T11:03:...|151.38450000000017|2017-05-08|       151.0|\n",
      "|Elevator07|  Elevator|2017-05-08T11:25:...|156.65510000000157|2017-05-08|       157.0|\n",
      "|Elevator03|  Elevator|2017-05-16T12:41:...|163.86150000000322|2017-05-16|       164.0|\n",
      "+----------+----------+--------------------+------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.functions import rint\n",
    "\n",
    "cdf_select = cdf.select(\"deviceId\",\"deviceType\",\"timestamp\", \"data.d.motorTemp\")\n",
    "cdf_date = cdf_select.withColumn(\"date\", substring('timestamp',1,10))\n",
    "cdf_mtemp = cdf_date.withColumn(\"motorTempInt\", rint('motorTemp'))\n",
    "\n",
    "cdf_mtemp.cache()\n",
    "\n",
    "if debug : cdf_mtemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Event Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code do the aggregation using groupBy and agg.\n",
    "The operations are documented here:\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+------------------+\n",
      "|  deviceId|deviceType|      date|    min(motorTemp)|    avg(motorTemp)|    max(motorTemp)|\n",
      "+----------+----------+----------+------------------+------------------+------------------+\n",
      "|Elevator02|  Elevator|2017-05-08|          163.7207|166.94495418060362|170.10070000000326|\n",
      "|Elevator09|  Elevator|2017-05-16|          159.3389|162.97127758112282|166.67890000000375|\n",
      "|Elevator05|  Elevator|2017-05-08|          161.2412|164.43060221402376|167.78120000000334|\n",
      "|Elevator01|  Elevator|2017-05-11|          150.5035|156.09050301205096|161.60350000000568|\n",
      "|Elevator08|  Elevator|2017-05-08|          151.5074|154.53478007380232|157.68740000000315|\n",
      "|Elevator06|  Elevator|2017-05-16|161.39589999999998|164.91468609310735|168.39590000000356|\n",
      "|Elevator06|  Elevator|2017-05-08|157.02169999999998|160.09710961408416| 163.1017000000031|\n",
      "|Elevator01|  Elevator|2017-05-08|          151.0245|172.97757808311005|             214.0|\n",
      "|Elevator08|  Elevator|2017-05-16|155.66889999999998| 159.2650487702477|162.80890000000363|\n",
      "|Elevator09|  Elevator|2017-05-08|151.21949999999998|154.44929264214213|157.65950000000328|\n",
      "|Elevator07|  Elevator|2017-05-08|          153.5751|156.73402250922678|159.95510000000326|\n",
      "|Elevator10|  Elevator|2017-05-11|            150.76|156.34268173258286|162.02000000000575|\n",
      "|Elevator10|  Elevator|2017-05-16|169.60629999999998|173.18122007104975| 176.8663000000037|\n",
      "|Elevator04|  Elevator|2017-05-16|151.98049999999998|155.64550299940197| 159.0805000000036|\n",
      "|Elevator03|  Elevator|2017-05-08|152.13879999999997| 155.2293771812096| 158.4388000000032|\n",
      "|Elevator10|  Elevator|2017-05-08|          162.1702| 165.3467211551393| 168.6302000000033|\n",
      "|Elevator02|  Elevator|2017-05-16|160.17739999999998|163.77602028302064| 167.4374000000037|\n",
      "|Elevator05|  Elevator|2017-05-11|          156.3576|172.28645425287763|             180.0|\n",
      "|Elevator07|  Elevator|2017-05-11|          162.6832|168.40398160919835|174.06320000000582|\n",
      "|Elevator04|  Elevator|2017-05-11|          158.1572|163.89473351206723| 169.2972000000057|\n",
      "+----------+----------+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "#cdf_mtemp.filter(cdf_mtemp.motorTempInt > 164).show()\n",
    "cdf_aggtemp = cdf_mtemp.groupBy('deviceId','deviceType','date').agg(min('motorTemp'),avg('motorTemp'),max('motorTemp'))\n",
    "if debug : cdf_aggtemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code aggregate the event data to determine the distribution of temperatures for each elevator by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------+-------------------+\n",
      "|  deviceId|deviceType|      date|motorTempInt|count(motorTempInt)|\n",
      "+----------+----------+----------+------------+-------------------+\n",
      "|Elevator08|  Elevator|2017-05-16|       161.0|                250|\n",
      "|Elevator02|  Elevator|2017-05-16|       161.0|                237|\n",
      "|Elevator08|  Elevator|2017-05-11|       157.0|                199|\n",
      "|Elevator07|  Elevator|2017-05-08|       156.0|                219|\n",
      "|Elevator07|  Elevator|2017-05-16|       168.0|                250|\n",
      "|Elevator06|  Elevator|2017-05-11|       165.0|                231|\n",
      "|Elevator02|  Elevator|2017-05-08|       167.0|                231|\n",
      "|Elevator07|  Elevator|2017-05-08|       160.0|                 88|\n",
      "|Elevator05|  Elevator|2017-05-11|       179.0|                 75|\n",
      "|Elevator10|  Elevator|2017-05-16|       177.0|                 87|\n",
      "|Elevator10|  Elevator|2017-05-16|       174.0|                237|\n",
      "|Elevator01|  Elevator|2017-05-16|       164.0|                225|\n",
      "|Elevator09|  Elevator|2017-05-16|       164.0|                231|\n",
      "|Elevator07|  Elevator|2017-05-08|       154.0|                194|\n",
      "|Elevator04|  Elevator|2017-05-08|       171.0|                 70|\n",
      "|Elevator02|  Elevator|2017-05-16|       165.0|                237|\n",
      "|Elevator09|  Elevator|2017-05-08|       156.0|                230|\n",
      "|Elevator09|  Elevator|2017-05-11|       167.0|                250|\n",
      "|Elevator03|  Elevator|2017-05-08|       154.0|                237|\n",
      "|Elevator05|  Elevator|2017-05-16|       156.0|                237|\n",
      "+----------+----------+----------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "cdf_distrtemp = cdf_mtemp.groupBy('deviceId','deviceType','date','motorTempInt').agg(count('motorTempInt'))\n",
    "if debug : cdf_distrtemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Aggregated Data to dashDB Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to connect this notebook to dashDB systems, modify the credentials cell here. To do so click \"Find and Add Data\" at top right of the screen, then select \"Connection\" and select \"Insert to code\" for the dashDB system of your choice. Make sure you have a dashDB connection set up in your project beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code is used to access your data and contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "\n",
    "credentials_dashdb = {\n",
    "    'host' : '<host>',\n",
    "    'port' : '50000',\n",
    "    'database' : 'BLUDB',\n",
    "    'jdbcurl': '<jdbc>',\n",
    "    'username': 'dash6769',\n",
    "    'password': '<password>'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to save the data to dashDB. The lines of code relevant for doing that have been borrowed from: https://apsportal.ibm.com/analytics/notebooks/55e7fe57-c002-4180-bfa3-83fc6466a875/view?access_token=1792df00fbbcf6df08f73bec0055b34e988d323eea59aafc6cd12d2a4ea86ed0\n",
    "Unlike mentioned in that tutorial, it is no longer neded to install pixiedust and a custom JDBC driver dialect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --user --upgrade pixiedust\n",
    "print(\"Please restart the kernel and then run through the notebook again (skipping this cell).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pixiedust\n",
    "print dir(pixiedust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops!\n"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "try:\n",
    "   cdf_aggtemp.write.jdbc(credentials_dashdb[\"jdbcurl\"], \"TEMP_DATAFRAME_AGGR_EVENT_DATA\", properties = {\"user\" : credentials_dashdb[\"username\"], \"password\" : credentials_dashdb[\"password\"]}, mode=\"overwrite\")\n",
    "except Py4JJavaError:\n",
    "   print \"Oops!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf_aggtemp.write.jdbc(credentials_dashdb[\"jdbcurl\"], \"TEMP_DATAFRAME_AGGR_EVENT_DATA\", properties = {\"user\" : credentials_dashdb[\"username\"], \"password\" : credentials_dashdb[\"password\"]}, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf_distrtemp.write.jdbc(credentials_dashdb[\"jdbcurl\"], \"TEMP_DATAFRAME_DISTR_EVENT_DATA\", properties = {\"user\" : credentials_dashdb[\"username\"], \"password\" : credentials_dashdb[\"password\"]}, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Aggregated Data to Target Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregated data has been stored in a temporary table named TEMP_DATAFRAME_AGGR_EVENT_DATA in dashDB. From there it needs to be moved to the table named ELEVATOR_EVENTS_AGGREGATED_BY_DAY that holds all aggregated data from previous runs of the ETL job. \n",
    "\n",
    "In this notebook we will use ibmdpy to create a connection to dashdb as described in: \n",
    "https://apsportal.ibm.com/analytics/notebooks/5a59ba9b-02b2-40e4-b955-9727cb68c88b/view?access_token=09240b783432f1a62004bcc82b48a7aed07afc401e2f94a77c7e087b74d8c053\n",
    "\n",
    "Having established the connection, an SQL statement will be submitted that merges the content of the table TEMP_DATAFRAME_DISTR_EVENT_DATA to the table ELEVATOR_EVENTS_AGGREGATED_BY_DAY suing the following SQL statement:\n",
    "\n",
    "MERGE INTO ELEVATOR_EVENTS_AGGREGATED_BY_DAY t\n",
    "USING (SELECT s.\"deviceId\" AS DEVICEID, s.\"deviceType\" AS DEVICETYPE, DATE(s.\"date\") AS DATE, ROUND(\"min(motorTemp)\",0) AS MINMOTORTEMP, ROUND(\"avg(motorTemp)\",0) AS AVGMOTORTEMP, ROUND(\"max(motorTemp)\",0) AS MAXMOTORTEMP FROM TEMP_DATAFRAME_AGGR_EVENT_DATA s) e ON (e.DATE=t.DATE AND e.DEVICEID = t.DEVICEID)\n",
    "WHEN MATCHED THEN \n",
    "    UPDATE SET t.MINMOTORTEMP = e.MINMOTORTEMP, t.AVGMOTORTEMP = e.AVGMOTORTEMP, t.MAXMOTORTEMP = e.MAXMOTORTEMP\n",
    "WHEN NOT MATCHED THEN \n",
    "    INSERT (DATE, DEVICEID, DEVICETYPE, MINMOTORTEMP, AVGMOTORTEMP, MAXMOTORTEMP) \n",
    "    VALUES (e.DATE, e.DEVICEID, e.DEVICETYPE, e.MINMOTORTEMP,e.AVGMOTORTEMP,e.MAXMOTORTEMP);\n",
    "    \n",
    "Notice that the merge makes the ETL job idempotent. Even if run several times it will produce the same result by updating existing records in the target database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ibmdbpy\n",
    "from ibmdbpy import IdaDataBase\n",
    "idadb = idadb = IdaDataBase(dsn=\"DASHDB;Database=BLUDB;Hostname=\" + credentials_dashdb[\"host\"] + \";Port=\" + credentials_dashdb[\"port\"] + \";PROTOCOL=TCPIP;UID=\" + credentials_dashdb[\"username\"] + \";PWD=\" + credentials_dashdb[\"password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE INTO ELEVATOR_EVENTS_AGGREGATED_BY_DAY t USING (SELECT s.\"deviceId\" AS DEVICEID, s.\"deviceType\" AS DEVICETYPE, DATE(s.\"date\") AS DATE, ROUND(\"min(motorTemp)\",0) AS MINMOTORTEMP, ROUND(\"avg(motorTemp)\",0) AS AVGMOTORTEMP, ROUND(\"max(motorTemp)\",0) AS MAXMOTORTEMP FROM TEMP_DATAFRAME_AGGR_EVENT_DATA s) e ON (e.DATE=t.DATE AND e.DEVICEID = t.DEVICEID) WHEN MATCHED THEN UPDATE SET t.MINMOTORTEMP = e.MINMOTORTEMP, t.AVGMOTORTEMP = e.AVGMOTORTEMP, t.MAXMOTORTEMP = e.MAXMOTORTEMP WHEN NOT MATCHED THEN INSERT (DATE, DEVICEID, DEVICETYPE, MINMOTORTEMP, AVGMOTORTEMP, MAXMOTORTEMP) VALUES (e.DATE, e.DEVICEID, e.DEVICETYPE, e.MINMOTORTEMP,e.AVGMOTORTEMP,e.MAXMOTORTEMP);\n",
      "Number of recs before: 30\n",
      "Number of recs after: 30\n"
     ]
    }
   ],
   "source": [
    "from ibmdbpy import IdaDataFrame\n",
    "\n",
    "numberofRecsBefore = idadb.ida_scalar_query(\"SELECT count(*) FROM ELEVATOR_EVENTS_AGGREGATED_BY_DAY\")\n",
    "\n",
    "query =         'MERGE INTO ELEVATOR_EVENTS_AGGREGATED_BY_DAY t '\n",
    "query = query + 'USING (SELECT s.\"deviceId\" AS DEVICEID, s.\"deviceType\" AS DEVICETYPE, DATE(s.\"date\") AS DATE, ROUND(\"min(motorTemp)\",0) AS MINMOTORTEMP, ROUND(\"avg(motorTemp)\",0) AS AVGMOTORTEMP, ROUND(\"max(motorTemp)\",0) AS MAXMOTORTEMP '\n",
    "query = query + 'FROM TEMP_DATAFRAME_AGGR_EVENT_DATA s) e ON (e.DATE=t.DATE AND e.DEVICEID = t.DEVICEID) '\n",
    "query = query + 'WHEN MATCHED THEN UPDATE SET t.MINMOTORTEMP = e.MINMOTORTEMP, t.AVGMOTORTEMP = e.AVGMOTORTEMP, t.MAXMOTORTEMP = e.MAXMOTORTEMP ' \n",
    "query = query + 'WHEN NOT MATCHED THEN INSERT (DATE, DEVICEID, DEVICETYPE, MINMOTORTEMP, AVGMOTORTEMP, MAXMOTORTEMP) VALUES (e.DATE, e.DEVICEID, e.DEVICETYPE, e.MINMOTORTEMP,e.AVGMOTORTEMP,e.MAXMOTORTEMP);'\n",
    "print query\n",
    "\n",
    "result = idadb.ida_query(query)\n",
    "\n",
    "numberofRecsAfter = idadb.ida_scalar_query(\"SELECT count(*) FROM ELEVATOR_EVENTS_AGGREGATED_BY_DAY\")\n",
    "\n",
    "print(\"Number of recs before: \" + str(numberofRecsBefore))\n",
    "print(\"Number of recs after: \" + str(numberofRecsAfter))\n",
    "\n",
    "idadb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
